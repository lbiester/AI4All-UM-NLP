{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYUhgh1IcCUG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd '/content/drive/My Drive/AI4All-UM-NLP'\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CRc94VqcASA"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import lil_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTubzneCcASD"
   },
   "source": [
    "# Classification\n",
    "\n",
    "Our final task will be to use the tools that we have explored to classify gender based on happiness. Along the way, we will see how to split data to train and test classifiers and how data is represented as input in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6Fr6VThcASE"
   },
   "source": [
    "## Splitting Data\n",
    "\n",
    "Before we train any classifiers, we need to split our data into a train set, dev set, and test set.\n",
    "\n",
    "Create three lists of writer IDs: train (80%), test (10%), and dev (10%). Make sure that these lists do not have any overlap, and contain all writers with their gender labeled as male or female. As you saw in section 1, we do not have very many authors whose gender is other, so it would be impossible to perform classification.\n",
    "\n",
    "Scikit-learn has a funciton, `train_test_split`, that will split data for you. Note that it only does a single split; think about how you can use it to create three distinct datasets. If you do not want to use scikit-learn, you may implement this yourself. However, for debugging, you should seed your random number generator, which will cause it to have the same results each time you use it. You can see the [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi0YbF9AK1M0"
   },
   "source": [
    "There are two ways that you should consider splitting the data\n",
    "\n",
    "1. Split by happy moment: create one list of happy moments, then split them into train, dev, and test\n",
    "1. Split by worker (more complex, but better): splitting by worker is better because you won't be training on workers who are in the test set. If, for instance, a father constantly mentions his son \"Gregory,\" the classifier might learn that \"Gregory\" is more commonly said by men, even though it is really just Gregory's father. If Gregory's father is in the test set as well as the train set, you will have higher accuracy than you should.  \n",
    "To prevent this, you can split __writers__ into train, dev, and test sets. Then, create a list of the corresponding happy moments for train, dev, and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU-XTf1icASE"
   },
   "source": [
    "Load your data (use the function for _joined_ data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bhs9fOJ0cASF"
   },
   "outputs": [],
   "source": [
    "joined_data = lib.load_joined_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6SRYm4acASH"
   },
   "source": [
    "Create a new list, `joined_data_clean`, that only contains happy moments where the author identifies as male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLuw9RIQcASH"
   },
   "outputs": [],
   "source": [
    "joined_data_clean = [hm for hm in joined_data if hm['gender'] in ['m', 'f']]\n",
    "all_writers = list(set([writer['wid'] for writer in lib.load_demographics() if writer['gender'] in ['m', 'f']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf0lUGTScASJ"
   },
   "source": [
    "Split your data into three separate lists: `train`, `dev`, and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgsE6JlqcASK"
   },
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are splitting by happy moment, you are done this section. If you are splitting by worker, use this cell to make train, test, and dev lists of _happy moments_ based on the splits of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWXUdcxgI78z"
   },
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3rhp4nycASL"
   },
   "source": [
    "## Defining a Baseline\n",
    "One good baseline is the _majority class_. This is defined as the percentage of data that comes from the most common class. In a classification problem, it is often the case that one class appears more frequently in the data than the other.\n",
    "\n",
    "The simplest baseline is random, which would be 50% on a binary classification task like ours. However, with unbalanced data, that does not take into account the fact that guessing the most common class 100% of the time would yield a higher baseline. What is our majority class baseline? Print it out, and be sure to compare your results to the baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvPp7OQtcASM"
   },
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "\n",
    "\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDtE-ZeOcASN"
   },
   "source": [
    "## First Feature: Counts\n",
    "We will first train our model by using counts of words as features. You should create a feature matrix (using numpy) with the following properties:\n",
    "* There is one row for each sentence\n",
    "* Each column is a count of the number of times that each word appears in that sentence\n",
    "\n",
    "You can think of this as a grid, where on the top you have words and on the side you have sentences.\n",
    "\n",
    "You should\n",
    "* Fill in the class `CountMatrix`. The two methods you will write, `fit_transform` and `transform` are analogous to\n",
    "terminology used in sklearn. `fit_transform` will create a new matrix based on the words in your sentence, while `transform` will create a matrix with the column -> word mapping that was used when you called `fit_transform`! Make sure that `transform` can only be called if `fit_transform` has already been called!\n",
    "* Think about what to do with unknown words. You can search online to see if you can find any solutions to this problem!\n",
    "\n",
    "You will need to use a sparse matrix from scipy to accomplish this without creating a data structure that is too big for colab. I would recommend using scipy's [lil_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html). Compared to some of the other sparse matrices, it is easy to construct in a similar way to how you would construct a matrix in numpy. There is a warning that \"to construct a matrix efficiently, make sure the items are pre-sorted by index, per row\" but in our case this does not seem to matter very much in terms of time, so do not worry about sorting if you don't want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQTyeQQ6gedz"
   },
   "source": [
    "*List of Lists Format (LIL)*\n",
    "\n",
    "Examples:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8ulfCD7iz7h"
   },
   "outputs": [],
   "source": [
    "#create an empty LIL matrix of 4 rows and 5 columns\n",
    "mtx = lil_matrix((4,5))\n",
    "print(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzg_xTOWibTU"
   },
   "outputs": [],
   "source": [
    "#create a random array data \n",
    "data = np.round(rand(2,3))\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPVCB2hDsYBL"
   },
   "outputs": [],
   "source": [
    "#initialize the LIL matrix with the random array and then print it. Do you notice something interesting? How does the lil_matrix differ from the original matrix?\n",
    "mtx[:2,[1,2,3]] = data\n",
    "print(\"Lil matrix:\")\n",
    "print(mtx)\n",
    "print(\"Original matrix:\")\n",
    "print(mtx.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aevpUeLi8xj"
   },
   "source": [
    "Now, create your `CountMatrix`\n",
    "\n",
    "Note: you can insert to your lil_matrix using the folowing:\n",
    "`matrix[x, y] = z`\n",
    "\n",
    "Hint (highlight text to see):<font color='white'>this is how you should initialize your count matrix: count_matrix = lil_matrix((len(sentences), len(self.word_to_int)), dtype=np.int64)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUZeM06OcASO"
   },
   "outputs": [],
   "source": [
    "class CountMatrix:\n",
    "    def __init__(self):\n",
    "        self.word_to_int = {}\n",
    "    \n",
    "    def fit_transform(self, sentences):\n",
    "        # this function should create a map from each unique token to a column number\n",
    "        # then it should convert this list of sentences into a matrix, and return that matrix\n",
    "        ### YOUR WORK HERE\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR WORK\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        # this should convert a list of sentences into a matrix, then return that matrix\n",
    "        \n",
    "        ### YOUR WORK HERE\n",
    "\n",
    "        \n",
    "        ### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ktUxfz_cASQ"
   },
   "source": [
    "Use your CountMatrix to create input and output variables for your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, define variables for the following: text for train/dev happy moments\n",
    "# use as many lines of code as you need, the following are just placeholders\n",
    "train_text = ???\n",
    "\n",
    "dev_text = ???\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbvvhSwgcASQ"
   },
   "outputs": [],
   "source": [
    "count_matrix = CountMatrix()\n",
    "train_input = count_matrix.fit_transform(train_text)\n",
    "dev_input = count_matrix.transform(dev_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train/dev output. This is just the gender variable for each HM in train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iROlrLYPcAST"
   },
   "source": [
    "Now that you have created your features, you can train your classifier. For this exercise, use the LogisticRegression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XX7VbzP5cAST"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# call fit on the model\n",
    "\n",
    "\n",
    "# test the model on dev set\n",
    "# call predict\n",
    "\n",
    "\n",
    "\n",
    "# use metrics.accuracy_score to calculate accuracy. usage is: metrics.accuracy_score(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_Ax_odNcASV"
   },
   "source": [
    "## Adding a new feature: length\n",
    "We saw in section 2 that length of happiness reflections can differ for men and women. What happens if we add this feature in addition to counts? Does it help with our performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJ9bzNLmcASW"
   },
   "source": [
    "Create feature vectors that include only the length of the sequence. Create them as a list. The cell below the next one will convert them to a lil_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count up length features. use nltk tokenizer\n",
    "def count_lengths(text):\n",
    "    lengths = []\n",
    "    ### YOUR WORK HERE\n",
    "    \n",
    "    \n",
    "    ### END YOUR WORK\n",
    "    return lengths\n",
    "\n",
    "train_lengths = count_lengths(train_text)\n",
    "dev_lengths = count_lengths(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3OgjvmBcASX"
   },
   "outputs": [],
   "source": [
    "length_feature_train = lil_matrix(train_lengths).reshape(-1, 1)\n",
    "length_feature_dev = lil_matrix(dev_lengths).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VZ2SDFQcASY"
   },
   "source": [
    "Next, use [`hstack` from scipy.sparse](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.hstack.html) to combine them with your count features\n",
    "\n",
    "Note: you'll need to make the length features a sparse matrix as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tO08skqcASZ"
   },
   "outputs": [],
   "source": [
    "# combine the features together into one matrix for dev and train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dKRdBYscASb"
   },
   "source": [
    "Finally, train the model again with the new features to see if the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uee7WTIFcASb"
   },
   "outputs": [],
   "source": [
    "# follow what you did above to train your model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ek3P4R92cASd"
   },
   "source": [
    "## TF-IDF Counts\n",
    "TF-IDF stands for term frequency-inverse document frequency. It is a way of weighting words such that words have the highest weights if they are _common_ in a single document but _uncommon_ in the full set of documents. This means that words like \"a\" would have a lower weight, even if they appear frequently in a single document, because they are so common overall. You can think of a document as a happy moment sentence in our case!\n",
    "\n",
    "[Wikipedia gives a very complete description of how TF-IDF is calculated](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition), and you should refer to this when implementing the method. If you have questions about notation, please ask an instructor or a neighbor, as it is a bit tricky!\n",
    "\n",
    "Fill in the class `TFIDFMatrix`, which will contain TF-IDF values instead of raw counts. Please feel free to add additional helper methods to this class as you calculate TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUPyhtPrcASd"
   },
   "outputs": [],
   "source": [
    "class TFIDFMatrix:\n",
    "    def __init__(self):\n",
    "        self.word_to_int = {}\n",
    "    \n",
    "    def fit_transform(self, sentences):\n",
    "        ### YOUR WORK HERE\n",
    "        # create a word to column mapping\n",
    "        # this should be a lot like what you did before!\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR WORK\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        ### YOUR WORK HERE\n",
    "        if len(self.word_to_int) == 0:\n",
    "            raise Exception('Must call fit_transform before transform!')\n",
    "        \n",
    "        # calculate document frequency\n",
    "\n",
    "        \n",
    "        \n",
    "        # calculate tf-idf\n",
    "\n",
    "        \n",
    "        \n",
    "        # return tfidf-matrix at the end\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR WORK\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        # this function will be used later, don't worry too much about it!\n",
    "        return [x[0] for x in sorted(self.word_to_int.items(), key=lambda x: x[1])]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmMaK2OCcASf"
   },
   "source": [
    "Use your TFIDFMatrix to create input and output variables for your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eleeEgifcASf"
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = TFIDFMatrix()\n",
    "train_input = tfidf_matrix.fit_transform(train_text)\n",
    "\n",
    "dev_input = tfidf_matrix.transform(dev_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1v0p8-_cASh"
   },
   "source": [
    "Finally, train your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvC_qHr1cASh"
   },
   "outputs": [],
   "source": [
    "# do the same thing that you have done before\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RREududScASj"
   },
   "source": [
    "## Examining Model Weights\n",
    "In addition to succeeding at classification, we can look at the _weights_ of our classifier. This will tell us which words are most influential in making correct classifications!\n",
    "\n",
    "This helps us to determine what makes men happy and not women, and vice-versa.\n",
    "\n",
    "The model weights are stored as `model.coef_`. They will line up with the feature names in your vectorizer, which you can find by running `vectorizer.get_feature_names()`.\n",
    "\n",
    "Once you have the weights for all features, you can sort by coefficient to find the largest and smallest coefficients, which will link to men and women.\n",
    "\n",
    "Do you see any similarities between the coefficient lists and your word clouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNnntRWocASk"
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf_matrix.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "\n",
    "combo = []\n",
    "for i in range(len(feature_names)):\n",
    "    combo.append((feature_names[i], coefficients[i]))\n",
    "    \n",
    "sorted_combos = sorted(combo, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kc8VJ05QcASm"
   },
   "outputs": [],
   "source": [
    "sorted_combos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgjkH2RXcASp"
   },
   "outputs": [],
   "source": [
    "sorted_combos[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73Yl9xoccASr"
   },
   "source": [
    "## Modifying your Features\n",
    "After seeing the results of top weights, is there anything that you would change with how you created your features? Is there any additional pre-processing that you might do?\n",
    "\n",
    "If so, try making these modifications in your CountMatrix and TFIDFMatrix, and see if it improves your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCp4x4krcASs"
   },
   "source": [
    "## Your Turn: Other Features?\n",
    "Are there any other features that you think could help your classifier performance? If so, try adding them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vydJOhllcASs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3ab6FkycASu"
   },
   "source": [
    "## Testing!\n",
    "\n",
    "Once you're done playing around wiht different features, you can test your best classifier on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gFXWwOecASu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-OG0b76cASw"
   },
   "source": [
    "## Reflection\n",
    "These results could tell us that different things make men and women happy. What else could they tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfkKNRELcASx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1svy15NicAS2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "4_Classifying_Happiness.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
