{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtW0wGEdbhTY"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd '/content/drive/My Drive/AI4All-UM-NLP'\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccimXCGTbelz"
   },
   "outputs": [],
   "source": [
    "import lib\n",
    "import nltk\n",
    "import spacy\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhhxdMS_D6A1"
   },
   "source": [
    "# Is happiness seasonal?\n",
    "Determine whether or not people mention different seasons more in relation to their happiness. We provide the list of seasons. Which season makes people happiest?\n",
    "\n",
    "Before we figure this out, we're going to explore an important NLP tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rCgy8z36v0iN"
   },
   "source": [
    "## Tokenization\n",
    "We used tokenization in the last notebook, but we didn't really learn much about it. Before we continue to look at seasons, let's learn a bit more about tokenization.\n",
    "\n",
    "Tokenization is one of the first pre-processing steps in NLP. It is the process of splitting a string of text into individual tokens. You can think of a token as an individual word or a piece of punctuation.\n",
    "\n",
    "You may have seen the method `split`, which is used for strings in python. It takes a string and splits it into words, based on white space.\n",
    "\n",
    "\n",
    "NLTK, a python toolkit for NLP, has its own function, `nltk.word_tokenize`. This is a \"smarter\" version of tokenization. Let's see why. We show how to tokenize the sentence below using both methods. What is the main difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7Ccum7ow6K0"
   },
   "outputs": [],
   "source": [
    "sentence = 'When I went to the store today, I bought apples, bananas, and oranges!'\n",
    "\n",
    "# print out the list returned using `split`\n",
    "print(sentence.split())\n",
    "\n",
    "\n",
    "# print out the list returned using `word_tokenize`\n",
    "print(nltk.word_tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjXGYtgIxK0U"
   },
   "source": [
    "What major differences do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NdFrsQJEXfX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgnyiGcPEZFt"
   },
   "source": [
    "Now, we will create a list of all tokens in the dataset using `nltk.word_tokenize`. We will make each token lowercase. If you don't know how to do that, there is an example below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: this is how you make a token lowercase!\n",
    "\n",
    "token = 'Sunday'\n",
    "\n",
    "print(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scklNnpgbel2"
   },
   "outputs": [],
   "source": [
    "joined_data = lib.load_joined_data()\n",
    "\n",
    "# create your list of tokens!\n",
    "all_tokens = []\n",
    "for happy_moment in joined_data:\n",
    "  for word in nltk.word_tokenize(happy_moment['cleaned_hm']):\n",
    "    # append the word to all_tokens and make it lowercase!\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4amd5dC_Eqxj"
   },
   "source": [
    "Now, write a function count_seasons, that takes in your tokens as input and prints out the count for each season. You can use the seasons list provided above!\n",
    "\n",
    "There are two options for how you can count\n",
    "1. Count using a dictionary. If you don't remember how, look at the python worksheet.\n",
    "2. Use individual variables to count each season, then make a dictionary at the end!\n",
    "\n",
    "The code is currently \"set up\" for the first option, but you should feel free to do the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVckg3Iqbel6"
   },
   "outputs": [],
   "source": [
    "def count_seasons(all_tokens, seasons):\n",
    "  counts = {}\n",
    "  for token in all_tokens:\n",
    "    for season in seasons:\n",
    "      # check if the token and season is equivalent\n",
    "      # if they are, increase counts for this season\n",
    "      # this will require two ifs, one else, and 5 lines of code\n",
    "      ???\n",
    "  print(counts)\n",
    "  \n",
    "# here we call the function on all_tokens\n",
    "seasons = ['spring', 'summer', 'fall', 'winter']\n",
    "count_seasons(all_tokens, seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msxDB64fbel8"
   },
   "source": [
    "### The magic of preprocesssing\n",
    "nltk's word tokenize algorithm is trained to handle special cases like punctuation. However, we saw that a simple \"tokenizer\" is python's `string.split` function, which splits a string on white space.\n",
    "\n",
    "Instead of using `nltk.word_tokenize`, we will use `string.split` this time, and run our count_seasons function on the new resulting set of tokens. Make sure to make all of your tokens **lower case** again! Do you get different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpNyUJM_bel9"
   },
   "outputs": [],
   "source": [
    "split_tokens = []\n",
    "for happy_moment in joined_data:\n",
    "  for word in happy_moment['cleaned_hm'].split():\n",
    "    # append the word to split_tokens and make it lowercase!\n",
    "    ???\n",
    "\n",
    "    \n",
    "# count seasons - are your results the same? different?\n",
    "seasons = ['spring', 'summer', 'fall', 'winter']\n",
    "count_seasons(split_tokens, seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98QT5L0bbel-"
   },
   "source": [
    "How about if you don't use `lower` to ignore case? Write out the loop one more time, but don't use `lower` to ignore case. Do your results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNGtaDYIbel_"
   },
   "outputs": [],
   "source": [
    "split_tokens_cases = []\n",
    "for happy_moment in joined_data:\n",
    "  for word in happy_moment['cleaned_hm'].split():\n",
    "    # append the word to split_tokens_cases, but don't make it lowercase!\n",
    "    ???\n",
    "\n",
    "    \n",
    "# count seasons - are your results the same? different?\n",
    "# this time, we will use uppercase season names, since we are likely to have uppercase seasons in our data\n",
    "seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
    "count_seasons(split_tokens_cases, seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5H5W6IgbemB"
   },
   "source": [
    "Which results seem most \"correct\" to you? Which pre-processing method is the most robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go-_NrDjFCjM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rgvROjQbemC"
   },
   "source": [
    "# What kinds of things are people happy about?\n",
    "To go beyond the simple example with seasons, we will explore what kinds of things people are happy about. In particular, we will explore two areas:\n",
    "1. What purchases are people happy with (i.e. can money buy happiness)?\n",
    "1. Who are people happy to do things with?\n",
    "\n",
    "In the end, we will see if mention people or purchases more in their happy moments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tq_0WJrxbemC"
   },
   "source": [
    "### Parsing sentences\n",
    "\n",
    "To find out what people have bought, you'll be using spaCy's sentence parser. If you aren't familiar, here is an example parse tree, generated using [this site](http://nlpviz.bpodgursky.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/parse2.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the parse tree for the sentence \"I went to the store to buy some blue jeans.\" Different parse trees will have slightly different structures - sometimes more specific tags, like NNS (plural noun) are used, while sometimes only more general tags like N (noun) will be used.\n",
    "\n",
    "To begin, we'll play around with the spacy parser to find the NP that is associated with the word \"buy\" in our example sentence. Make sure that you actually use the parse tree structure, as it will become important later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFW_4F3cbemD"
   },
   "source": [
    "We will loop through the noun chunks to find the one whose \"head\" is \"buy.\" [The noun chunks documentation](https://spacy.io/usage/linguistic-features#noun-chunks) shows how to do this. Return all noun chunks for which the head is in the passed in list `buy_list`. The contents of this list should be verbs that have something to do with buying things!\n",
    "\n",
    "One thing to note to understand the code below is that noun chunks are _spans_ and individual words are _tokens_ in spaCy. This is why we need to use the `text` attribute; they aren't strings!!\n",
    "\n",
    "Here's the documentation for span and token objects:\n",
    "* [Spans](https://spacy.io/api/span)\n",
    "* [Tokens](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if something is in a list\n",
    "The following code is how you check if a string (or any type of data!) is in a list. This will be useful when you write your function.\n",
    "\n",
    "We can also check if something is _not_ in a list using `not in`, as shown below.\n",
    "\n",
    "Think about what you think the output should be before running this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'monday'\n",
    "\n",
    "y = 'saturday'\n",
    "\n",
    "weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "\n",
    "print(x in weekdays)\n",
    "\n",
    "print(y in weekdays)\n",
    "\n",
    "print(y not in weekdays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fill in the rest of the function, writing out the necessary if statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_CqDEidbemD"
   },
   "outputs": [],
   "source": [
    "sentence = 'I went to the store to buy some blue jeans'\n",
    "simple_buy_list = ['buy']\n",
    "\n",
    "def get_things_bought(document, buy_list):\n",
    "  things_bought = []\n",
    "    \n",
    "  # load spacy, and parse your document with spacy\n",
    "  nlp = spacy.load('en', max_length=2000000)\n",
    "  parsed = nlp(document)\n",
    "    \n",
    "  # now, we find all of the relevant noun chunks\n",
    "  for noun_chunk in parsed.noun_chunks:\n",
    "    root = noun_chunk.root\n",
    "    head = root.head\n",
    "\n",
    "    # check that part-of-speech of head is 'VERB', and that the text of the head is in the buy_list\n",
    "    # to get POS: head.pos_\n",
    "    # to get text: head.text\n",
    "    # to check if something is in buy_list: x in buy_list\n",
    "    \n",
    "    ### YOUR WORK HERE\n",
    "    if ???:\n",
    "      # if this condition is true, we will add the text from the noun chunk to things_bought\n",
    "      things_bought.append(noun_chunk.text)\n",
    "    ### END YOUR WORK\n",
    "            \n",
    "  return things_bought\n",
    "\n",
    "# This should print ['some blue jeans']\n",
    "print(get_things_bought(sentence, simple_buy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yEn1pgybemG"
   },
   "source": [
    "## Counting Purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBd4QIsxbemH"
   },
   "source": [
    "Now that you've finished the get_things_bought function, let's put everything together on our actual dataset.\n",
    "\n",
    "We'll start by loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bcefw0iCbemH"
   },
   "outputs": [],
   "source": [
    "# load happy moments and pre-process data\n",
    "# for this, we will use only 1/8 of the data, because spaCy parsing is very slow.\n",
    "eigth_index = len(joined_data) // 8\n",
    "hms = []\n",
    "for hm in joined_data[:eigth_index]:\n",
    "  hms.append(hm['cleaned_hm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Text\n",
    "In addition to hms, you will want to have a version of the happy moments that includes all of the sentences in a single string. Between the sentences, you can use newlines (`\\n`). Here's an example of how `join` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_things = ['chocolate', 'skittles', 'starburst', 'm&m']\n",
    "\n",
    "joined = '\\n'.join(some_things)\n",
    "\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the text from hms together to create `hms_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hms_text = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an expanded `buy_list`. Remember, the contents of this list should be verbs that have something to do with buying things!\n",
    "\n",
    "Consider different verb forms (like \"bought\") in addition to synonyms (like \"purchase\"). Try to have at least 6 words in your list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5kFQu8KbemK"
   },
   "outputs": [],
   "source": [
    "# define your buy list\n",
    "### YOUR WORK HERE\n",
    "buy_list = ???\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to count every purchase that has been mentioned. Code is provided to print the dictionary sorted **in descending order.**\n",
    "\n",
    "Hint: try calling your function with the full document instead of individual sentences. Individual sentences are not needed by spaCy, and this will make your code run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_purchased = get_things_bought(hms_text, buy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9DFPuu6bemM"
   },
   "outputs": [],
   "source": [
    "# count things\n",
    "# things_purchased will be a list of strings. Count the number of occurrences of each string using a dictionary\n",
    "# if you don't remember how, look at the python worksheet\n",
    "thing_counts = {}\n",
    "\n",
    "### YOUR WORK HERE!\n",
    "\n",
    "\n",
    "### END YOUR WORK!\n",
    "\n",
    "\n",
    "# sort\n",
    "thing_counts_sorted = sorted(thing_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# print\n",
    "for thing, count in thing_counts_sorted:\n",
    "  print(thing, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBwOhAFobemO"
   },
   "source": [
    "You might notice that some of the most common words here are not in fact things that people have bought. An obvious example is the common word \"I\"\n",
    "\n",
    "One sentence where \"I\" is pulled out of the parse tree is the following:\n",
    "```\n",
    "I bought a new TV\n",
    "```\n",
    "\n",
    "We want to filter out \"I\" since it is not really the thing that the worker bought.\n",
    "\n",
    "Modify your code to take a list of blacklist words that you define. Make sure you think about case as you work on this. Once you've added the blacklist, be creative! Add anything else that you think will help with your performance, like checking for lowercase!\n",
    "\n",
    "The skeleton of the code you wrote above is copied here for you to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define blacklist\n",
    "### YOUR WORK HERE\n",
    "blacklist = ???\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbtGnk0rbemP"
   },
   "outputs": [],
   "source": [
    "def get_things_bought_blacklist(document, buy_list, blacklist):\n",
    "  things_bought = []\n",
    "    \n",
    "  # load spacy, and parse your document with spacy\n",
    "  nlp = spacy.load('en', max_length=2000000)\n",
    "  parsed = nlp(document)\n",
    "    \n",
    "  # now, we find all of the relevant noun chunks\n",
    "  for noun_chunk in parsed.noun_chunks:\n",
    "    root = noun_chunk.root\n",
    "    head = root.head\n",
    "\n",
    "    # check that part-of-speech of head is 'VERB', and that the text of the head is in the buy_list\n",
    "    # to get POS: head.pos_\n",
    "    # to get text: head.text\n",
    "    # to check if something is in buy_list: x in buy_list\n",
    "    \n",
    "    ### YOUR WORK HERE\n",
    "    # first if is same as before: POS and buy_list\n",
    "    if ???:\n",
    "      # this time, we need to check that NOTHING in the noun_chunk is in the blacklist\n",
    "      # this is a bit complex because we need to check for all blacklisted words\n",
    "      bad = False\n",
    "      for word in blacklist:\n",
    "        # we want bad to be true if the word is in the noun chunk\n",
    "        # to do this, use the `in` operator, and make sure to check with noun_chunk.text\n",
    "        if ???:\n",
    "          bad = True\n",
    "      if not bad:\n",
    "        things_bought.append(noun_chunk.text)\n",
    "    ### END YOUR WORK\n",
    "            \n",
    "  return things_bought\n",
    "  \n",
    "things_purchased_blacklist = get_things_bought_blacklist(hms_text, buy_list, blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same thing you did before... count things, and run code to sort and print\n",
    "# things_purchased will be a list of strings. Count the number of occurrences of each string using a dictionary\n",
    "# if you don't remember how, look at the python worksheet\n",
    "thing_counts_blacklist = {}\n",
    "\n",
    "### YOUR WORK HERE!\n",
    "\n",
    "\n",
    "### END YOUR WORK!\n",
    "\n",
    "\n",
    "# sort\n",
    "thing_counts_blacklist_sorted = sorted(thing_counts_blacklist.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# print\n",
    "for thing, count in thing_counts_blacklist_sorted:\n",
    "  print(thing, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BSjFut_bemQ"
   },
   "source": [
    "What purchases seem to make people the most happy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1JwjFgXbemR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwJxk5PqbemS"
   },
   "source": [
    "Finally, count up and print the total number of purchases mentioned in this chunk of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w50yi_EXbemS"
   },
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "total_purchases = ???\n",
    "\n",
    "\n",
    "### END YOUR WORK\n",
    "print(total_purchases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-OVRhrObemU"
   },
   "source": [
    "### Counting Personal Interactions\n",
    "In addition to purchases, we want to count other people who are mentioned in the dataset. This will be a fairly simple pattern-matching exercise, like what we did for seasons. However, we will do a little bit of (our own) parsing to get some ideas!\n",
    "\n",
    "Much of the time, people are mentioned using a possessive like *my*. Go through all of the sentences, searching for the word *my*. Count up occurrences of words that appear after *my*. This might give you some ideas about what to look for! We will print out the top 200 words in the list in order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye4TE3OMbemU"
   },
   "outputs": [],
   "source": [
    "# count words that occur after my\n",
    "after_my = {}\n",
    "for sentence in hms:\n",
    "  if 'my' in sentence:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # only look at len(tokens) - 1.\n",
    "    # question: why would we do this? think about your answer.\n",
    "    for i in range(len(tokens) - 1):\n",
    "      current_token = tokens[i]\n",
    "      next_token = tokens[i + 1]\n",
    "      # in the if statement, check if the current token is 'my'\n",
    "      if ???:\n",
    "        # increment the count of the next token in the if statement.\n",
    "        if next_token in after_my:\n",
    "          ???\n",
    "        else:\n",
    "          ???\n",
    "\n",
    "print(after_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c58KjnPabemW"
   },
   "outputs": [],
   "source": [
    "# print words that commonly occur after my, in order\n",
    "# sort\n",
    "after_my_sorted = sorted(after_my.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print\n",
    "for word, count in after_my_sorted[:200]:\n",
    "  print(word, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3TCmz3ubemY"
   },
   "source": [
    "Now that you have some ideas, build your list of personal relationships (call it `relationships`), and write a function `count_relationships`. This will be a lot like `count_seasons`, but you don't need to have multiple variables to count - you only need to count all of the relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rpmMCsbbemY"
   },
   "outputs": [],
   "source": [
    "relationships = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yQMBy6Gbemb"
   },
   "outputs": [],
   "source": [
    "def count_relationships(hms, relationships):\n",
    "  count = 0\n",
    "  ### YOUR WORK HERE\n",
    "  for sentence in hms:\n",
    "    for relationship in relationships:\n",
    "      # check if the relationpship is mentioned in the sentence\n",
    "      # you can use 'in' just like you would for a list.\n",
    "      if ???:\n",
    "        # increase the count\n",
    "        ???\n",
    "\n",
    "  ### END YOUR WORK\n",
    "  return count\n",
    "  \n",
    "count_relationships(hms, relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45a4_xI7bemk"
   },
   "source": [
    "Do people mention relationships that they are happy with more, or people? What does this tell us about the general cause of happiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MpBVq21beml"
   },
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have extra time\n",
    "Consider updating your `count_relationships` to see which types of relationships people mention most often.\n",
    "\n",
    "Try to prevent double-counting, so that one relationship is only counted a single time in `count_relationships`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_Exploring_Text.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
