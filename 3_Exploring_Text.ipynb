{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Exploring_Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AtW0wGEdbhTY",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    %cd '/content/drive/My Drive/AI4All-UM-NLP'\n",
        "\n",
        "    import nltk\n",
        "    nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ccimXCGTbelz",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import lib\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import itertools\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IhhxdMS_D6A1"
      },
      "source": [
        "# Is happiness seasonal?\n",
        "Determine whether or not people mention different seasons more in relation to their happiness. We provide the list of seasons. Which season makes people happiest?\n",
        "\n",
        "Before we figure this out, we're going to explore an important NLP tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCgy8z36v0iN",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "We used tokenization in the last notebook, but we didn't really learn much about it. Before we continue to look at seasons, let's learn a bit more about tokenization.\n",
        "\n",
        "Tokenization is one of the first pre-processing steps in NLP. It is the process of splitting a string of text into individual tokens. You can think of a token as an individual word or a piece of punctuation.\n",
        "\n",
        "You may have seen the method `split`, which is used for strings in python. It takes a string and splits it into words, based on white space.\n",
        "\n",
        "\n",
        "NLTK, a python toolkit for NLP, has its own function, `nltk.word_tokenize`. This is a \"smarter\" version of tokenization. Let's see why. Tokenize the sentence below using both methods. What is the main difference?\n",
        "\n",
        "\n",
        "To call `split` on a string, write `string.split()`\n",
        "To call `word_tokenize` on a string, write `nltk.word_tokenize(string)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Ccum7ow6K0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'When I went to the store today, I bought apples, bananas, and oranges!'\n",
        "\n",
        "# print out the list returned using `split`\n",
        "print(sentence.split())\n",
        "\n",
        "# print out the list returned using `word_tokenize`\n",
        "print(nltk.word_tokenize(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjXGYtgIxK0U",
        "colab_type": "text"
      },
      "source": [
        "What major differences do you see?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NdFrsQJEXfX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgnyiGcPEZFt",
        "colab_type": "text"
      },
      "source": [
        "Now, create a list of all tokens in the dataset. If you get stuck, you can look at what we did in the last notebook. Make sure to use `nltk.word_tokenize`, _and_ make each token lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "scklNnpgbel2",
        "colab": {}
      },
      "source": [
        "joined_data = lib.load_joined_data()\n",
        "\n",
        "def get_all_tokens(joined_data):\n",
        "    tokens = []\n",
        "    for hm in joined_data:\n",
        "        tokens.extend(nltk.word_tokenize(hm['hm_text'].lower()))\n",
        "    return tokens\n",
        "\n",
        "all_tokens = get_all_tokens(joined_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C4OFl48Cbel4",
        "colab": {}
      },
      "source": [
        "seasons = ['spring', 'summer', 'fall', 'winter']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4amd5dC_Eqxj",
        "colab_type": "text"
      },
      "source": [
        "Now, write a function count_seasons, that takes in your tokens as input and prints out the count for each season. You can use the seasons list provided above!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LVckg3Iqbel6",
        "colab": {}
      },
      "source": [
        "def count_seasons(all_tokens):\n",
        "    counts = Counter()\n",
        "    for token in all_tokens:\n",
        "        if token in seasons:\n",
        "            counts[token] += 1\n",
        "    print(counts)\n",
        "count_seasons(all_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "msxDB64fbel8"
      },
      "source": [
        "### The magic of preprocesssing\n",
        "nltk's word tokenize algorithm is trained to handle special cases like punctuation. However, we saw that a simple \"tokenizer\" is python's `string.split` function, which splits a string on white space.\n",
        "\n",
        "Write a new function, `get_all_tokens_split`, where you create your list of tokens using `string.split` instead of `nltk.word_tokenize`. Make sure to make all of your tokens **lower case** again! Do you get different results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rpNyUJM_bel9",
        "colab": {}
      },
      "source": [
        "def get_all_tokens_split(joined_data):\n",
        "    tokens = []\n",
        "    for hm in joined_data:\n",
        "        tokens.extend(hm['hm_text'].lower().split())\n",
        "    return tokens\n",
        "\n",
        "all_tokens_split = get_all_tokens_split(joined_data)\n",
        "\n",
        "count_seasons(all_tokens_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "98QT5L0bbel-"
      },
      "source": [
        "How about if you don't use `lower` to ignore case? Write one more function, `get_all_tokens_split_case_sensitive`, which does the same thing as `get_all_tokens_split` but doesn't ignore case. Check the results of `count_seasons` again. This time, we'll use upper case for our seasons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PNGtaDYIbel_",
        "colab": {}
      },
      "source": [
        "seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
        "\n",
        "def get_all_tokens_split_case_sensitive(joined_data):\n",
        "    tokens = []\n",
        "    for hm in joined_data:\n",
        "        tokens.extend(hm['hm_text'].split())\n",
        "    return tokens\n",
        "\n",
        "all_tokens_split_case_sensitive = get_all_tokens_split_case_sensitive(joined_data)\n",
        "\n",
        "count_seasons(all_tokens_split_case_sensitive)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z5H5W6IgbemB"
      },
      "source": [
        "Which results seem most \"correct\" to you? Which pre-processing method is the most robust?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go-_NrDjFCjM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1rgvROjQbemC"
      },
      "source": [
        "## What kinds of things are people happy about?\n",
        "To go beyond the simple example with seasons, we will explore what kinds of things people are happy about. In particular, we will explore two areas:\n",
        "1. What purchases are people happy with (i.e. can money buy happiness)?\n",
        "1. Who are people happy to do things with?\n",
        "\n",
        "In the end, we will see if mention people or purchases more in their happy moments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tq_0WJrxbemC"
      },
      "source": [
        "### Parsing sentences\n",
        "\n",
        "To find out what people have bought, you'll be using spaCy's sentence parser. If you aren't familiar, here is an example parse tree, generated using [this site](http://nlpviz.bpodgursky.com/):\n",
        "\n",
        "<img src=\"img/parse2.png\" width=\"500\">\n",
        "\n",
        "This is the parse tree for the sentence \"I went to the store to buy some blue jeans.\" Different parse trees will have slightly different structures - sometimes more specific tags, like NNS (plural noun) are used, while sometimes only more general tags like N (noun) will be used.\n",
        "\n",
        "To begin, we'll play around with the spacy parser to find the NP that is associated with the word \"buy\" in our example sentence. Make sure that you actually use the parse tree structure, as it will become important later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFW_4F3cbemD"
      },
      "source": [
        "You'll want to loop through the noun chunks to find the one whose \"head\" is \"buy.\" [The noun chunks documentation](https://spacy.io/usage/linguistic-features#noun-chunks) should give you a good idea of how to do this. Return all noun chunks for which the head is in the passed in list `buy_list`.\n",
        "\n",
        "It should be clear from the example how to get the text attribute from a noun chunk - do note that noun chunks are _spans_ and individual words are _tokens_ in spaCy. This means that you should always make sure to get the text if that's waht you want!\n",
        "\n",
        "Here's the documentation for span and token objects:\n",
        "* [Spans](https://spacy.io/api/span)\n",
        "* [Tokens](https://spacy.io/api/token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_CqDEidbemD",
        "colab": {}
      },
      "source": [
        "sentence = 'I went to the store to buy some blue jeans'\n",
        "simple_buy_list = ['buy']\n",
        "\n",
        "def get_things_bought(document, buy_list):\n",
        "    things_bought = []\n",
        "    \n",
        "    # load spacy, and parse your document with spacy\n",
        "    ### YOUR WORK HERE\n",
        "    nlp = spacy.load('en', max_length=2000000)\n",
        "    parsed = nlp(document)\n",
        "    ### END YOUR WORK\n",
        "    \n",
        "    # now, find all of the relevant noun chunks\n",
        "    # make sure you return their text!\n",
        "    ### YOUR WORK HERE\n",
        "    for noun_chunk in parsed.noun_chunks:\n",
        "        root = noun_chunk.root\n",
        "        head = root.head\n",
        "        \n",
        "        if head.pos_ == 'VERB' and head.text in buy_list:\n",
        "            things_bought.append(noun_chunk.text)\n",
        "            \n",
        "    ### END YOUR WORK\n",
        "    return things_bought\n",
        "\n",
        "# This should print ['some blue jeans']\n",
        "print(get_things_bought(sentence, simple_buy_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7yEn1pgybemG"
      },
      "source": [
        "### Counting Purchases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YBd4QIsxbemH"
      },
      "source": [
        "Now that you've written the get_things_bought function, let's put everything together on our actual dataset. Create a dictionary to count every purchase that has been mentioned, and then print the results **in descending order.** Be sure to print the count and the noun chunk.\n",
        "\n",
        "You will also want to extend your buy list. You will want to think of synonyms, in addition to different forms of the verb \"to buy\".\n",
        "\n",
        "Hint: try calling your function with the full document instead of individual sentences. Individual sentences are not needed by spaCy, and this will make your code run much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bcefw0iCbemH",
        "colab": {}
      },
      "source": [
        "# load happy moments\n",
        "# because parsing is slow, you should define a few variables here\n",
        "# 1. full_hms: this is a list of all happy moment texts in the dataset\n",
        "# 2. eigth_hms: this is 1/8 of the happy moments in the dataset. we will use this when our code is resource-intensive\n",
        "# 4. hms_text: this is eigth_hms, but we will join it together in a string! combine all strings with the \\n (newline)\n",
        "#    character.\n",
        "### YOUR WORK HERE\n",
        "hms = lib.load_happy_moments()\n",
        "full_hms = [hm['cleaned_hm'] for i, hm in enumerate(hms)]\n",
        "eigth_hms = [hm['cleaned_hm'] for i, hm in enumerate(hms) if i < len(hms) / 8]\n",
        "hms_text = '\\n'.join(eigth_hms)\n",
        "### END YOUR WORK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5kFQu8KbemK",
        "colab": {}
      },
      "source": [
        "# define your buy list\n",
        "### YOUR WORK HERE\n",
        "buy_list = ['buy', 'bought', 'purchase', 'purchased', 'purchasing', 'buying', 'order', 'ordering']\n",
        "### END YOUR WORK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d9DFPuu6bemM",
        "colab": {}
      },
      "source": [
        "# find purchases\n",
        "# this might take a few minutes to run, don't worry if it does!\n",
        "# hint: you will want to sort the list to print in descending order. \n",
        "# Ask for help if you don't know how to do this, or look it up online!\n",
        "### YOUR WORK HERE\n",
        "things_bought = {}\n",
        "things = get_things_bought(hms_text, buy_list)\n",
        "for thing in things:\n",
        "    if thing not in things_bought:\n",
        "        things_bought[thing] = 0\n",
        "    things_bought[thing] += 1\n",
        "things_sorted = sorted(things_bought.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for thing, count in things_sorted:\n",
        "    print(thing, count)\n",
        "### END YOUR WORK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PBwOhAFobemO"
      },
      "source": [
        "You might notice that some of the most common words here are not in fact things that people have bought. An obvious example is the common word \"I\"\n",
        "\n",
        "One sentence where \"I\" is pulled out of the parse tree is the following:\n",
        "```\n",
        "I bought a new TV\n",
        "```\n",
        "\n",
        "In this specific case, I think that the spaCy parser is doing something wrong my marking \"I\" as a child of \"bought\", (<span style=\"color:red\">proofreaders: do you agree?</span>) however, we can still filter it out. A more legitimate failure is from the sentence \"I bought my father a bicycle\", in which \"father\" really does belong under \"bought\" in the tree, but is not the thing that was bought.\n",
        "\n",
        "Modify your code to take a list of blacklist words that you define. Make sure you think about case as you work on this. Once you've added the blacklist, be creative! Add anything else that you think will help with your performance.\n",
        "\n",
        "You can copy your code down here to work on this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbtGnk0rbemP",
        "colab": {}
      },
      "source": [
        "# define blacklist\n",
        "### YOUR WORK HERE\n",
        "things_blacklisted = ['my', 'I', 'me', 'i', 'it', 'his', 'her', 'their', 'we', 'us', 'he', 'him', 'she', 'her', 'they', 'them']\n",
        "### END YOUR WORK\n",
        "\n",
        "def get_things_bought_blacklist(document, buy_list, blacklist):\n",
        "    things_bought = []\n",
        "    \n",
        "    # load spacy, and parse your document with spacy\n",
        "    ### YOUR WORK HERE\n",
        "    nlp = spacy.load('en', max_length=2000000)\n",
        "    parsed = nlp(document)\n",
        "    ### END YOUR WORK\n",
        "    \n",
        "    # now, find all of the relevant noun chunks\n",
        "    # make sure you return their text!\n",
        "    ### YOUR WORK HERE\n",
        "    for noun_chunk in parsed.noun_chunks:\n",
        "        root = noun_chunk.root\n",
        "        head = root.head\n",
        "        \n",
        "        if head.pos_ == 'VERB' and head.text in buy_list:\n",
        "            bad = False\n",
        "            for word in blacklist:\n",
        "                if word in noun_chunk.text.lower():\n",
        "                    bad = True\n",
        "            if not bad:\n",
        "                things_bought.append(noun_chunk.text)\n",
        "            \n",
        "    ### END YOUR WORK\n",
        "    return things_bought\n",
        "\n",
        "### YOUR WORK HERE\n",
        "things_bought = {}\n",
        "things = get_things_bought_blacklist(hms_text, buy_list, things_blacklisted)\n",
        "for thing in things:\n",
        "    if thing not in things_bought:\n",
        "        things_bought[thing] = 0\n",
        "    things_bought[thing] += 1\n",
        "things_sorted = sorted(things_bought.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for thing, count in things_sorted:\n",
        "    print(thing, count)\n",
        "### END YOUR WORK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4BSjFut_bemQ"
      },
      "source": [
        "What purchases seem to make people the most happy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L1JwjFgXbemR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwJxk5PqbemS"
      },
      "source": [
        "Finally, count up and print the total number of purchases mentioned in this chunk of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w50yi_EXbemS",
        "colab": {}
      },
      "source": [
        "### YOUR WORK HERE\n",
        "total_purchases = sum(things_bought.values())\n",
        "### END YOUR WORK\n",
        "print(total_purchases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V-OVRhrObemU"
      },
      "source": [
        "### Counting Personal Interactions\n",
        "In addition to purchases, we want to count other people who are mentioned in the dataset. This will be a fairly simple pattern-matching exercise, like what we did for seasons. However, we will do a little bit of (our own) parsing to get some ideas!\n",
        "\n",
        "Much of the time, people are mentioned using a possessive like *my*. Go through all of the sentences, searching for the word *my*. Count up occurrences of words that appear after *my*. This might give you some ideas about what to look for! Print the list out in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ye4TE3OMbemU",
        "colab": {}
      },
      "source": [
        "# TODO: save tokenized sentences for speed for students!\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "### YOUR WORK HERE\n",
        "from collections import Counter\n",
        "after_my = Counter()\n",
        "for sentence in eigth_hms:\n",
        "    if 'my' in sentence:\n",
        "        tokens = nlp(sentence)\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token.text == 'my' and i != len(tokens) - 1:\n",
        "                next_token = tokens[i + 1].text\n",
        "                after_my[next_token] += 1\n",
        "        \n",
        "#         counted += 1\n",
        "#         print(counted)\n",
        "    \n",
        "# print all words that occur with \"my\"\n",
        "### END YOUR WORK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c58KjnPabemW",
        "colab": {}
      },
      "source": [
        "after_my.most_common(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f3TCmz3ubemY"
      },
      "source": [
        "Now that you have some ideas, build your list of personal relationships (call it `relationships`), and write a function `count relationships`.\n",
        "\n",
        "Make sure that you do not double count a happy moment with multiple relationships mentioned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rpmMCsbbemY",
        "colab": {}
      },
      "source": [
        "relationships = ['family', 'wife', 'husband', 'son', 'daughter', 'girlfriend', 'friend', 'parents', 'mother', 'sister',\n",
        "                'mom', 'boyfriend', 'brother', 'kids', 'boss', 'dad', 'grandpa', 'nephew', 'uncle', 'partner',\n",
        "                'niece', 'baby', 'children', 'child', 'spouse', 'cousin', 'ex', 'neighbor', 'fiance', 'daughters',\n",
        "                'granddaughter', 'students', 'aunt', 'roommate', 'coworkers']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5yQMBy6Gbemb",
        "colab": {}
      },
      "source": [
        "def count_relationships(hms, relationships):\n",
        "    count = 0\n",
        "    ### YOUR WORK HERE\n",
        "    for sentence in eigth_hms:\n",
        "        for relationship in relationships:\n",
        "            if relationship in sentence:\n",
        "                count += 1\n",
        "                break\n",
        "    ### END YOUR WORK\n",
        "    return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LtjtN0robeme",
        "colab": {}
      },
      "source": [
        "count_relationships(hms, relationships)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "45a4_xI7bemk"
      },
      "source": [
        "Do people mention relationships that they are happy with more, or people? What does this tell us about the general cause of happiness?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1MpBVq21beml",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}