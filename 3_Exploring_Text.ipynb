{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import lib\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is happiness seasonal?\n",
    "Determine whether or not people mention different seasons more in relation to their happiness. We provide the list of seasons. Which season makes people happiest?\n",
    "\n",
    "Write the `get_all_tokens` function with `nltk.word_tokenize` to get all of the tokens from `joined_data`. Remember, you wrote code to do this before in section 2. Feel free to copy it over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = lib.load_joined_data()\n",
    "\n",
    "def get_all_tokens(joined_data):\n",
    "    tokens = []\n",
    "    for hm in joined_data:\n",
    "        tokens.extend(nltk.word_tokenize(hm['hm_text'].lower()))\n",
    "    return tokens\n",
    "\n",
    "all_tokens = get_all_tokens(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['spring', 'summer', 'fall', 'winter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_seasons(all_tokens):\n",
    "    counts = Counter()\n",
    "    for token in all_tokens:\n",
    "        if token in seasons:\n",
    "            counts[token] += 1\n",
    "    print(counts)\n",
    "count_seasons(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The magic of preprocesssing\n",
    "nltk's word tokenize algorithm is trained to handle special cases like punctuation. However, a simple \"tokenizer\" is python's `string.split` function, which splits a string on white space.\n",
    "\n",
    "Write a new function, `get_all_tokens_split`, where you create your list of tokens using `string.split` instead of `nltk.word_tokenize`. Do you get different results? Look at the CSV files (<span style=\"color:red\">TODO:</span> make sure this is possible in collab) - can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens_split(joined_data):\n",
    "    tokens = []\n",
    "    for hm in joined_data:\n",
    "        tokens.extend(hm['hm_text'].lower().split())\n",
    "    return tokens\n",
    "\n",
    "all_tokens_split = get_all_tokens_split(joined_data)\n",
    "\n",
    "count_seasons(all_tokens_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about if you don't use `lower` to ignore case? Write one more function, `get_all_tokens_split_case_sensitive`, which does the same thing as `get_all_tokens_split` but doesn't ignore case. Check the results of `count_seasons` again. This time, we'll use upper case for our seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
    "\n",
    "def get_all_tokens_split_case_sensitive(joined_data):\n",
    "    tokens = []\n",
    "    for hm in joined_data:\n",
    "        tokens.extend(hm['hm_text'].split())\n",
    "    return tokens\n",
    "\n",
    "all_tokens_split_case_sensitive = get_all_tokens_split_case_sensitive(joined_data)\n",
    "\n",
    "count_seasons(all_tokens_split_case_sensitive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results seem most \"correct\" to you? Which pre-processing method is the most robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kinds of things are people happy about?\n",
    "To go beyond the simple example with seasons, we will explore what kinds of things people are happy about. In particular, we will explore two areas:\n",
    "1. What purchases are people happy with (i.e. can money buy happiness)?\n",
    "1. Who are people happy to do things with?\n",
    "\n",
    "In the end, we will see if mention people or purchases more in their happy moments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing sentences\n",
    "\n",
    "To find out what people have bought, you'll be using spaCy's sentence parser. If you aren't familiar, here is an example parse tree, generated using [this site](http://nlpviz.bpodgursky.com/):\n",
    "\n",
    "<img src=\"img/parse2.png\" width=\"500\">\n",
    "\n",
    "This is the parse tree for the sentence \"I went to the store to buy some blue jeans.\" Different parse trees will have slightly different structures - sometimes more specific tags, like NNS (plural noun) are used, while sometimes only more general tags like N (noun) will be used.\n",
    "\n",
    "To begin, we'll play around with the spacy parser to find the NP that is associated with the word \"buy\" in our example sentence. Make sure that you actually use the parse tree structure, as it will become important later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll want to loop through the noun chunks to find the one whose \"head\" is \"buy.\" [The noun chunks documentation](https://spacy.io/usage/linguistic-features#noun-chunks) should give you a good idea of how to do this. Return all noun chunks for which the head is in the passed in list `buy_list`.\n",
    "\n",
    "It should be clear from the example how to get the text attribute from a noun chunk - do note that noun chunks are _spans_ and individual words are _tokens_ in spaCy. This means that you should always make sure to get the text if that's waht you want!\n",
    "\n",
    "Here's the documentation for span and token objects:\n",
    "* [Spans](https://spacy.io/api/span)\n",
    "* [Tokens](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I went to the store to buy some blue jeans'\n",
    "simple_buy_list = ['buy']\n",
    "\n",
    "def get_things_bought(document, buy_list):\n",
    "    things_bought = []\n",
    "    \n",
    "    # load spacy, and parse your query with spacy\n",
    "    ### YOUR WORK HERE\n",
    "    nlp = spacy.load('en', max_length=2000000)\n",
    "    parsed = nlp(document)\n",
    "    ### END YOUR WORK\n",
    "    \n",
    "    # now, find all of the relevant noun chunks\n",
    "    # make sure you return their text!\n",
    "    ### YOUR WORK HERE\n",
    "    for noun_chunk in parsed.noun_chunks:\n",
    "        root = noun_chunk.root\n",
    "        head = root.head\n",
    "        \n",
    "        if head.pos_ == 'VERB' and head.text in buy_list:\n",
    "            things_bought.append(noun_chunk.text)\n",
    "            \n",
    "    ### END YOUR WORK\n",
    "    return things_bought\n",
    "\n",
    "# This should print ['some blue jeans']\n",
    "print(get_things_bought(sentence, simple_buy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've written the get_things_bought function, let's put everything together on our actual dataset. Create a dictionary to count every purchase that has been mentioned, and then print the results descending order. Be sure to print the count and the noun chunk.\n",
    "\n",
    "You will also want to extend your buy list. You will want to think of synonyms, in addition to different forms of the verb \"to buy\".\n",
    "\n",
    "Hint: try calling your function with the full document instead of individual sentences. Individual sentences are not needed by spaCy, and this will make your code run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load happy moments\n",
    "# because parsing is slow, you should define a few variables here\n",
    "# 1. full_hms: this is a list of all happy moment texts in the dataset\n",
    "# 2. eigth_hms: this is 1/8 of the happy moments in the dataset. we will use this when our code is resource-intensive\n",
    "# 4. hms_text: this is eigth_hms, but we will join it together in a string! combine all strings with the \\n (newline)\n",
    "#    character.\n",
    "### YOUR WORK HERE\n",
    "hms = lib.load_happy_moments()\n",
    "full_hms = [hm['cleaned_hm'] for i, hm in enumerate(hms)]\n",
    "eigth_hms = [hm['cleaned_hm'] for i, hm in enumerate(hms) if i < len(hms) / 8]\n",
    "hms_text = '\\n'.join(eigth_hms)\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your buy list\n",
    "### YOUR WORK HERE\n",
    "buy_list = ['buy', 'bought', 'purchase', 'purchased', 'purchasing', 'buying', 'order', 'ordering']\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find purchases\n",
    "# this might take a few minutes to run, don't worry if it does!\n",
    "### YOUR WORK HERE\n",
    "things_bought = {}\n",
    "things = get_things_bought(hms_text, buy_list)\n",
    "for thing in things:\n",
    "    if thing not in things_bought:\n",
    "        things_bought[thing] = 0\n",
    "    things_bought[thing] += 1\n",
    "things_sorted = sorted(things_bought.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for thing, count in things_sorted:\n",
    "    print(thing, count)\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that some of the most common words here are not in fact things that people have bought. An obvious example is the common word \"I\"\n",
    "\n",
    "One sentence where \"I\" is pulled out of the parse tree is the following:\n",
    "```\n",
    "I bought a new TV\n",
    "```\n",
    "\n",
    "In this specific case, I think that the spaCy parser is doing something wrong my marking \"I\" as a child of \"bought\", (<span style=\"color:red\">proofreaders: do you agree?</span>) however, we can still filter it out. A more legitimate failure is from the sentence \"I bought my father a bicycle\", in which \"father\" really does belong under \"bought\" in the tree, but is not the thing that was bought.\n",
    "\n",
    "Modify your code to take a list of blacklist words that you define. Make sure you think about case as you work on this. Once you've added the blacklist, be creative! Add anything else that you think will help with your performance.\n",
    "\n",
    "You can copy your code down here to work on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define blacklist\n",
    "### YOUR WORK HERE\n",
    "things_blacklisted = ['my', 'I', 'me', 'i', 'it', 'his', 'her', 'their', 'we', 'us', 'he', 'him', 'she', 'her', 'they', 'them']\n",
    "### END YOUR WORK\n",
    "\n",
    "def get_things_bought_blacklist(document, buy_list, blacklist):\n",
    "    things_bought = []\n",
    "    \n",
    "    # load spacy, and parse your query with spacy\n",
    "    ### YOUR WORK HERE\n",
    "    nlp = spacy.load('en', max_length=2000000)\n",
    "    parsed = nlp(document)\n",
    "    ### END YOUR WORK\n",
    "    \n",
    "    # now, find all of the relevant noun chunks\n",
    "    # make sure you return their text!\n",
    "    ### YOUR WORK HERE\n",
    "    for noun_chunk in parsed.noun_chunks:\n",
    "        root = noun_chunk.root\n",
    "        head = root.head\n",
    "        \n",
    "        if head.pos_ == 'VERB' and head.text in buy_list:\n",
    "            bad = False\n",
    "            for word in blacklist:\n",
    "                if word in noun_chunk.text.lower():\n",
    "                    bad = True\n",
    "            if not bad:\n",
    "                things_bought.append(noun_chunk.text)\n",
    "            \n",
    "    ### END YOUR WORK\n",
    "    return things_bought\n",
    "\n",
    "### YOUR WORK HERE\n",
    "things_bought = {}\n",
    "things = get_things_bought_blacklist(hms_text, buy_list, things_blacklisted)\n",
    "for thing in things:\n",
    "    if thing not in things_bought:\n",
    "        things_bought[thing] = 0\n",
    "    things_bought[thing] += 1\n",
    "things_sorted = sorted(things_bought.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for thing, count in things_sorted:\n",
    "    print(thing, count)\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What purchases seem to make people the most happy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, count up and print the total number of purchases mentioned in this chunk of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "total_purchases = sum(things_bought.values())\n",
    "### END YOUR WORK\n",
    "print(total_purchases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Personal Interactions\n",
    "In addition to purchases, we want to count other people who are mentioned in the dataset. This will be a fairly simple pattern-matching exercise, like what we did for seasons. However, we will do a little bit of (our own) parsing to get some ideas!\n",
    "\n",
    "Much of the time, people are mentioned using a possessive like *my*. Go through all of the sentences, searching for the word *my*. Count up occurrences of words that appear after *my*. This might give you some ideas about what to look for! Print the list out in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save tokenized sentences for speed for students!\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "### YOUR WORK HERE\n",
    "from collections import Counter\n",
    "after_my = Counter()\n",
    "for sentence in eigth_hms:\n",
    "    if 'my' in sentence:\n",
    "        tokens = nlp(sentence)\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.text == 'my' and i != len(tokens) - 1:\n",
    "                next_token = tokens[i + 1].text\n",
    "                after_my[next_token] += 1\n",
    "        \n",
    "#         counted += 1\n",
    "#         print(counted)\n",
    "    \n",
    "# print all words that occur with \"my\"\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_my.most_common(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some ideas, build your list of personal relationships (call it `relationships`), and write a function `count relationships`.\n",
    "\n",
    "Make sure that you do not double count a happy moment with multiple relationships mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = ['family', 'wife', 'husband', 'son', 'daughter', 'girlfriend', 'friend', 'parents', 'mother', 'sister',\n",
    "                'mom', 'boyfriend', 'brother', 'kids', 'boss', 'dad', 'grandpa', 'nephew', 'uncle', 'partner',\n",
    "                'niece', 'baby', 'children', 'child', 'spouse', 'cousin', 'ex', 'neighbor', 'fiance', 'daughters',\n",
    "                'granddaughter', 'students', 'aunt', 'roommate', 'coworkers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_relationships(hms, relationships):\n",
    "    count = 0\n",
    "    ### YOUR WORK HERE\n",
    "    for sentence in eigth_hms:\n",
    "        for relationship in relationships:\n",
    "            if relationship in sentence:\n",
    "                count += 1\n",
    "                break\n",
    "    ### END YOUR WORK\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_relationships(hms, relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do people mention relationships that they are happy with more, or people? What does this tell us about the general cause of happiness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
