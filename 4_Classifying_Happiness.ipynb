{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lib\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Our final task will be to use the tools that we have explored to classify gender based on happiness. Along the way, we will see how to split data to train and test classifiers and how data is represented as input in NLP.\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> maybe we should have the students implement a simple classifier like NB, which is what the Stanford project does. We could do what we are doing here, using a classifier out-of-the-box, then have them implement their own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "\n",
    "Before we train any classifiers, we need to split our data into a train set, dev set, and test set.\n",
    "\n",
    "Create three lists of writer IDs: train (80%), test (10%), and dev (10%). Make sure that these lists do not have any overlap, and contain all writers with their gender labeled as male or female. As you saw in section 1, we do not have very many authors whose gender is other, so it would be impossible to perform classification.\n",
    "\n",
    "Scikit-learn has a funciton, `train_test_split`, that will split data for you. Note that it only does a single split; think about how you can use it to create three distinct datasets. If you do not want to use scikit-learn, you may implement this yourself. However, for debugging, you should seed your random number generator, which will cause it to have the same results each time you use it.\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> should we expect them to look up documentation on how to use these functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = lib.load_demographics()\n",
    "happy_moments = lib.load_happy_moments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = pd.merge(demographics, happy_moments, left_on='wid', right_on='wid')\n",
    "\n",
    "# only keep relevant columns for simplicity\n",
    "joined_data = joined_data[['gender', 'cleaned_hm']]\n",
    "\n",
    "# drop where gender is not m or f\n",
    "joined_data = joined_data[joined_data['gender'].isin(['m', 'f'])]\n",
    "\n",
    "\n",
    "train, temp = train_test_split(joined_data, test_size=.2, random_state=10)\n",
    "dev, test = train_test_split(temp, test_size=.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Baseline\n",
    "One good baseline is the _majority class_. In a classification problem, it is often the case that one class appears more frequently in the data than the other.\n",
    "\n",
    "The simplest baseline is random, which would be 50% on a binary classification task like ours. However, with unbalanced data, that does not take into account the fact that guessing the most common class 100% of the time would yield a higher baseline. What is our majority class baseline? Print it out, and be sure to compare your results to the baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(dev['gender'])\n",
    "print(class_counts.most_common()[0][1] / sum(class_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Feature: Counts\n",
    "We first train our model using sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). The count vectorizer represents a sentence by counting the number of times that each word appears. Each position in the vector represents one word.\n",
    "\n",
    "You should\n",
    "* Create a CountVectorizer\n",
    "* Create the input about output variables that will be used in your classifier  \n",
    "  Think about where you should use `transform`, `fit`, or `fit_transform`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_input = vectorizer.fit_transform(train['cleaned_hm'])\n",
    "train_output = train['gender']\n",
    "\n",
    "dev_input = vectorizer.transform(dev['cleaned_hm'])\n",
    "dev_output = dev['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created your features, you can train your classifier. For this exercise, use the LogisticRegression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(train_input, train_output)\n",
    "\n",
    "# test the model on dev set\n",
    "predictions = model.predict(dev_input)\n",
    "print(metrics.accuracy_score(predictions, dev_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a new feature: length\n",
    "We saw in section 2 that length of happiness reflections can differ for men and women. What happens if we add this feature in addition to counts? Does it help with our performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_feature_train = np.array([len(nltk.word_tokenize(x)) for x in train['cleaned_hm']]).reshape(-1, 1)\n",
    "length_feature_dev = np.array([len(nltk.word_tokenize(x)) for x in dev['cleaned_hm']]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find a nicer (faster) way to do this. Maybe provide it as a library function.\n",
    "combo_train = np.concatenate((train_input.todense(), length_feature_train), axis=1)\n",
    "combo_dev = np.concatenate((dev_input.todense(), length_feature_dev), axis=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(combo_train, train_output)\n",
    "predictions = model.predict(combo_dev)\n",
    "print(metrics.accuracy_score(predictions, dev_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer\n",
    "TF-IDF stands for term frequency-inverse document frequency. It is a way of weighting words such that words have the highest weights if they are _common_ in a single document but _uncommon_ in the full set of documents. This means that words like \"a\" would have a lower weight, even if they appear frequently in a single document, because they are so common overall.\n",
    "\n",
    "Create your features again, this time using the TfidfVectorizer. Do you see any change in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_input = vectorizer.fit_transform(train['cleaned_hm'])\n",
    "train_output = train['gender']\n",
    "\n",
    "dev_input = vectorizer.transform(dev['cleaned_hm'])\n",
    "dev_output = dev['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_input, train_output)\n",
    "predictions = model.predict(dev_input)\n",
    "print(metrics.accuracy_score(predictions, dev_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Model Weights\n",
    "In addition to succeeding at classification, we can look at the _weights_ of our classifier. This will tell us which words are most influential in making correct classifications!\n",
    "\n",
    "This helps us to determine what makes men happy and not women, and vice-versa.\n",
    "\n",
    "The model weights are stored as `model.coef_`. They will line up with the feature names in your vectorizer, which you can find by running `vectorizer.get_feature_names()`.\n",
    "\n",
    "Once you have the weights for all features, you can sort by coefficient to find the largest and smallest coefficients, which will link to men and women.\n",
    "\n",
    "Do you see any similarities between the coefficient lists and your word clouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "weight_df = pd.DataFrame({'Word': feature_names,\n",
    "                          'Coeff': coefficients})\n",
    "weight_df = weight_df.sort_values(['Coeff', 'Word'], ascending=[0, 1])\n",
    "weight_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.tail(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Other Features?\n",
    "Are there any other features that you think could help your classifier performance? If so, try adding them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add ngram features to CountVectorizer\n",
    "# maybe ask them to create their own counts matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
