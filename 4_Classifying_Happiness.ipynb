{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lib\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Our final task will be to use the tools that we have explored to classify gender based on happiness. Along the way, we will see how to split data to train and test classifiers and how data is represented as input in NLP.\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> maybe we should have the students implement a simple classifier like NB, which is what the Stanford project does. We could do what we are doing here, using a classifier out-of-the-box, then have them implement their own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "\n",
    "Before we train any classifiers, we need to split our data into a train set, dev set, and test set.\n",
    "\n",
    "Create three lists of writer IDs: train (80%), test (10%), and dev (10%). Make sure that these lists do not have any overlap, and contain all writers with their gender labeled as male or female. As you saw in section 1, we do not have very many authors whose gender is other, so it would be impossible to perform classification.\n",
    "\n",
    "Scikit-learn has a funciton, `train_test_split`, that will split data for you. Note that it only does a single split; think about how you can use it to create three distinct datasets. If you do not want to use scikit-learn, you may implement this yourself. However, for debugging, you should seed your random number generator, which will cause it to have the same results each time you use it. You can see the [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your data (use the function for _joined_ data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = lib.load_joined_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new list, `joined_data_clean`, that only contains happy moments where the author identifies as male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data_clean = [hm for hm in joined_data if hm['gender'] in ['m', 'f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split your data into three separate lists: `train`, `dev`, and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(joined_data_clean, test_size=.2, random_state=10)\n",
    "dev, test = train_test_split(temp, test_size=.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Baseline\n",
    "One good baseline is the _majority class_. In a classification problem, it is often the case that one class appears more frequently in the data than the other.\n",
    "\n",
    "The simplest baseline is random, which would be 50% on a binary classification task like ours. However, with unbalanced data, that does not take into account the fact that guessing the most common class 100% of the time would yield a higher baseline. What is our majority class baseline? Print it out, and be sure to compare your results to the baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR WORK HERE\n",
    "class_counts = Counter([hm['gender'] for hm in joined_data_clean])\n",
    "print(class_counts.most_common()[0][1] / sum(class_counts.values()))\n",
    "### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Feature: Counts\n",
    "We will first train our model by using counts of words as features. You should create a feature matrix (using numpy) with the following properties:\n",
    "* There is one row for each sentence\n",
    "* Each column is a count of the number of times that each word appears in that sentence\n",
    "\n",
    "You can think of this as a grid, where on the top you have words and on the side you have sentences.\n",
    "\n",
    "You should\n",
    "* Fill in the class `CountMatrix`. The two methods you will write, `fit_transform` and `transform` are analogous to\n",
    "wording used in sklearn. `fit_transform` will create a new matrix based on the words in your sentence, while `transform` will create a matrix with the column -> word mapping that was used when you called `fit_transform`! Make sure that `transform` can only be called if `fit_transform` has already been called!\n",
    "* Think about what to do with unknown words. You can search online to see if you can find any solutions to this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountMatrix:\n",
    "    def __init__(self):\n",
    "        ### YOUR WORK HERE\n",
    "        self.word_to_int = {}\n",
    "        ### END YOUR WORK\n",
    "    \n",
    "    def fit_transform(self, sentences):\n",
    "        ### YOUR WORK HERE\n",
    "        # create a word to column mapping\n",
    "        col = 0\n",
    "        for sentence in sentences:\n",
    "            for token in nltk.word_tokenize(sentence):\n",
    "                if token not in self.word_to_int:\n",
    "                    self.word_to_int[token] = col\n",
    "                    col += 1\n",
    "        \n",
    "        return self.transform(sentences)\n",
    "        ### END YOUR WORK\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        ### YOUR WORK HERE\n",
    "        count_matrix = np.zeros((len(sentences), len(self.word_to_int)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for token in nltk.word_tokenize(sentence):\n",
    "                if token in self.word_to_int:\n",
    "                    count_matrix[i][self.word_to_int[token]] += 1\n",
    "        return count_matrix\n",
    "        ### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your CountMatrix to create input and output variables for your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = CountMatrix()\n",
    "train_input = count_matrix.fit_transform([hm['hm_text'] for hm in train])\n",
    "dev_input = count_matrix.transform([hm['hm_text'] for hm in dev])\n",
    "\n",
    "train_output = [hm['gender'] for hm in train]\n",
    "dev_output = [hm['gender'] for hm in dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created your features, you can train your classifier. For this exercise, use the LogisticRegression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(train_input, train_output)\n",
    "\n",
    "# test the model on dev set\n",
    "predictions = model.predict(dev_input)\n",
    "print(metrics.accuracy_score(predictions, dev_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a new feature: length\n",
    "We saw in section 2 that length of happiness reflections can differ for men and women. What happens if we add this feature in addition to counts? Does it help with our performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature vectors that include only the length of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_feature_train = np.array([len(nltk.word_tokenize(hm['hm_text'])) for hm in train]).reshape(-1, 1)\n",
    "length_feature_dev = np.array([len(nltk.word_tokenize(hm['hm_text'])) for hm in dev]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `np.concatenate` to conbine them with your count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_train = np.concatenate((train_input, length_feature_train), axis=1)\n",
    "combo_dev = np.concatenate((dev_input, length_feature_dev), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the model again with the new features to see if the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(combo_train, train_output)\n",
    "predictions = model.predict(combo_dev)\n",
    "print(metrics.accuracy_score(predictions, dev_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Counts\n",
    "TF-IDF stands for term frequency-inverse document frequency. It is a way of weighting words such that words have the highest weights if they are _common_ in a single document but _uncommon_ in the full set of documents. This means that words like \"a\" would have a lower weight, even if they appear frequently in a single document, because they are so common overall. You can think of a document as a happy moment sentence in our case!\n",
    "\n",
    "[Wikipedia gives a very complete description of how TF-IDF is calculated](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition), and you should refer to this when implementing the method. If you have questions about notation, please ask an instructor or a neighbor, as it is a bit tricky!\n",
    "\n",
    "Fill in the class `TFIDFMatrix`, which will contain TF-IDF values instead of raw counts. Please feel free to add additional helper methods to this class as you calculate TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFMatrix:\n",
    "    def __init__(self):\n",
    "        self.word_to_int = {}\n",
    "    \n",
    "    def fit_transform(self, sentences):\n",
    "        ### YOUR WORK HERE\n",
    "        # create a word to column mapping\n",
    "        col = 0\n",
    "        for sentence in sentences:\n",
    "            for token in nltk.word_tokenize(sentence):\n",
    "                if token not in self.word_to_int:\n",
    "                    self.word_to_int[token] = col\n",
    "                    col += 1\n",
    "        \n",
    "        return self.transform(sentences)\n",
    "        ### END YOUR WORK\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        ### YOUR WORK HERE\n",
    "        if len(self.word_to_int) == 0:\n",
    "            raise Exception('Must call fit_transform before transform!')\n",
    "        \n",
    "        # calculate document frequency\n",
    "        document_frequency = {}\n",
    "        for sentence in sentences:\n",
    "            for token in set(nltk.word_tokenize(sentence)):\n",
    "                if token not in document_frequency:\n",
    "                    document_frequency[token] = 0\n",
    "                document_frequency[token] += 1\n",
    "                    \n",
    "        # calculate tf-idf\n",
    "        tfidf_matrix = np.zeros((len(sentences), len(self.word_to_int)))\n",
    "        n_documents = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            token_counts = {}\n",
    "            for token in nltk.word_tokenize(sentence):\n",
    "                if token in self.word_to_int:\n",
    "                    if token not in token_counts:\n",
    "                        token_counts[token] = 0\n",
    "                    token_counts[token] += 1\n",
    "                        \n",
    "            for token in set(token_counts):\n",
    "                tf = token_counts[token] / sum(token_counts.values())\n",
    "                idf = np.log(n_documents / (1 + document_frequency[token]))\n",
    "                tfidf_value = tf * idf\n",
    "                tfidf_matrix[i][self.word_to_int[token]] = tfidf_value\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return [x[0] for x in sorted(self.word_to_int.items(), key=lambda x: x[1])]\n",
    "            \n",
    "        ### END YOUR WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your TFIDFMatrix to create input and output variables for your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = TFIDFMatrix()\n",
    "train_input = tfidf_matrix.fit_transform([hm['hm_text'] for hm in train])\n",
    "\n",
    "dev_input = tfidf_matrix.transform([hm['hm_text'] for hm in dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_input, train_output)\n",
    "predictions = model.predict(dev_input)\n",
    "print(metrics.accuracy_score(predictions, dev_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Model Weights\n",
    "In addition to succeeding at classification, we can look at the _weights_ of our classifier. This will tell us which words are most influential in making correct classifications!\n",
    "\n",
    "This helps us to determine what makes men happy and not women, and vice-versa.\n",
    "\n",
    "The model weights are stored as `model.coef_`. They will line up with the feature names in your vectorizer, which you can find by running `vectorizer.get_feature_names()`.\n",
    "\n",
    "Once you have the weights for all features, you can sort by coefficient to find the largest and smallest coefficients, which will link to men and women.\n",
    "\n",
    "Do you see any similarities between the coefficient lists and your word clouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_matrix.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "\n",
    "combo = []\n",
    "for i in range(len(feature_names)):\n",
    "    combo.append((feature_names[i], coefficients[i]))\n",
    "    \n",
    "sorted_combos = sorted(combo, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combos[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying your Features\n",
    "After seeing the results of top weights, is there anything that you would change with how you created your features? Is there any additional pre-processing that you might do?\n",
    "\n",
    "If so, try making these modifications in your CountMatrix and TFIDFMatrix, and see if it improves your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Other Features?\n",
    "Are there any other features that you think could help your classifier performance? If so, try adding them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing!\n",
    "\n",
    "Once you're done playing around wiht different features, you can test your best classifier on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "These results could tell us that different things make men and women happy. What else could they tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
